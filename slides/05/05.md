title: NPFL129, Lecture 5
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Derivation of Softmax,<br> Kernel Methods

## Milan Straka

### November 02, 2019

---
section: LagrangeMult
# Lagrange Multipliers – Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

Given a funtion $J(→x)$, we can find a maximum with respect to a vector
$→x ∈ ℝ^d$, by investigating the critical points $∇_→x J(→x) = 0$.

~~~
Consider now finding maximum subject to a constraint $g(→x) = 0$.

~~~
- Note that $∇_→x g(→x)$ is orthogonal to the surface of the constraint, because
  if $→x$ and a nearby point $→x+→ε$ lie on the surface, from the Taylor
  expansion $g(→x+→ε) ≈ g(→x) + →ε^T ∇_→x g(→x)$ we get $→ε^T ∇_→x g(→x) ≈ 0$.

~~~
- In the seeked maximum, $∇_→x f(→x)$ must also be orthogonal to the constraint
  surface (or else moving in the direction of the derivative would increase the
  value).

~~~
- Therefore, there must exist $λ$ such that $∇_→x f + λ∇_→x g = 0$.

---
# Lagrange Multipliers – Equality Constraints

![w=28%,f=right](lagrange_equalities.svgz)

We therefore introduce the _Lagrangian function_
$$L(→x, λ) ≝ f(→x) + λg(→x).$$

~~~
We can then find the maximum under the constraint by inspecting critical points
of $L(→x, λ)$ with respect to both $→x$ and $λ$:
- $\frac{∂L}{∂λ} = 0$ leads to $g(→x)=0$;
- $\frac{∂L}{∂→x} = 0$ is the previously derived $∇_→x f + λ∇_→x g = 0$.



---
section: VariationalCalc
class: dbend
# Calculus of Variations

Many optimization techniques depend on minimizing a function $J(→w)$ with
respect to a vector $→w ∈ ℝ^d$, by investigating the critical points $∇_→w J(→w) = 0$.

~~~
A function of a function, $J[f]$, is known as a **functional**, for example
entropy $H[⋅]$.

~~~
Similarly to partial derivatives, we can take **functional derivatives** of
a functional $J[f]$ with respect to individual values $f(→x)$ for all points
$→x$. The functional derivative of $J$ with respect to a function $f$ in a point
$→x$ is denoted as
$$\frac{∂}{∂f(→x)} J.$$

~~~
For this course, we use only the following theorem stating that for
all differentiable functions $f$ and differentiable functions $g(y=f(→x), →x)$ with
continuous derivatives, it holds that
$$\frac{∂}{∂f(→x)} ∫g(f(→x), →x) \d→x = \frac{∂}{∂y} g(y, →x).$$

---
section: VariationalCalc
class: dbend
# Calculus of Variations

An intuitive view is to think about $f(→x)$ as a vector of uncountably many
elements (for every value $→x)$. In this interpretation the result is analogous
to computing partial derivatives of a vector $→w ∈ ℝ^d$:
$$\frac{∂}{∂w_i} ∑_j g(w_j, →x) = \frac{∂}{∂w_i} g(w_i, →x).$$
$$\frac{∂}{∂f(→x)} ∫g(f(→x), →x) \d→x = \frac{∂}{∂y} g(y, →x).$$

---
section: NAsMaxEnt
class: dbend
# Function with Maximum Entropy

What distribution over $ℝ$ maximizes entropy $H[p] = -𝔼_x \log p(x)$?

~~~
For continuous values, the entropy is an integral $H[p] = -∫p(x) \log p(x) \d x$.

~~~
We cannot just maximize $H$ with respect to a function $p$, because:
- the result might not be a probability distribution – we need to add
  a constraint that $∫p(x) \d x=1$;
~~~
- the problem is underspecified because a distribution can be shifted without
  changing entropy – we add a constraint $𝔼[x] = μ$;
~~~
- because entropy increases as variance increases, we ask which distribution
  with a _fixed_ variance $σ^2$ has maximum entropy – adding a constraint
  $\Var(x) = σ^2$.

---
class: dbend
# Function with Maximum Entropy

Lagrangian of all the constraints and the entropy function is
$$L(p; μ, σ^2) = λ_1 \Big(∫p(x) \d x - 1\Big) + λ_2 \big(𝔼[x] - μ\big) + λ_3\big(\Var(x) - σ^2\big) + H[p].$$

~~~
By expanding all definitions to integrals, we get
$$\begin{aligned}
L(p; μ, σ^2) =& ∫\Big(λ_1 p(x) + λ_2 p(x) x + λ_3 p(x) (x - μ)^2 - p(x)\log p(x) \Big) \d x - \\
              & -λ_1 - μ λ_2 - σ^2λ_3.
\end{aligned}$$

~~~
The functional derivative of $L$ is:
$$\frac{∂}{∂p(x)} L(p; μ, σ^2) = λ_1 + λ_2 x + λ_3 (x - μ)^2 - 1 - \log p(x) = 0.$$

---
class: dbend
# Function with Maximum Entropy
Rearrangint the functional derivative of $L$:

$$\frac{∂}{∂p(x)} L(p; μ, σ^2) = λ_1 + λ_2 x + λ_3 (x - μ)^2 - 1 - \log p(x) = 0.$$

we obtain
$$p(x) = \exp\Big(λ_1 + λ_2 x + λ_3 (x-μ)^2 - 1\Big).$$

~~~
We can verify that setting $λ_1 = 1 - \log σ \sqrt{2π}$, $λ_2=0$ and $λ_3=-1/(2σ^2)$
fulfils all the constraints, arriving at
$$p(x) = 𝓝(x; μ, σ^2).$$
