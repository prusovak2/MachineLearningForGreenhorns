title: NPFL129, Lecture 3
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Perceptron and Logistic Regression

## Milan Straka

### October 19, 2020

---
section: CV
# Cross-Validation

We already talked about a **train set** and a **test set**. Given that the main
goal of machine learning is to perform well on unseen data, the test set must
not be used during training nor hyperparameter selection. Ideally, it is hidden
to us altogether.

~~~
Therefore, to evaluate a machine learning model (for example to select model
architecture, features, or hyperparameter value), we normally need the
**validation** or a **development** set.

~~~
However, using a single development set might give us noisy results. To obtain
less noisy results (i.e., with smaller variance), we can use
**cross-validation**.

~~~
![w=48%,f=right](k-fold_cross_validation.svgz)

In cross-validation, we choose multiple validation sets from the training data,
and for every one, we train a model on the rest of the training data and
evaluate on the chosen validation sets. A commonly used strategy to choose
the validation sets is called **k-fold cross-validation**. Here the training set is partitioned
into $k$ subsets of approximately the same size, and each subset takes turn
to play a role of a validation set.

---
# Cross-Validation

An extreme case of the **k-fold cross-validation** is **leave-one-out
cross-validation**, where every element is considered a separate validation
set.

~~~
Computing leave-one-out cross-validation is usually extremely inefficient for
larger training sets, but in case of linear regression with L2 regularization,
it can be evaluated efficiently.
~~~
- If you are interested, see:

  _Ryan M. Rifkin and Ross A. Lippert: Notes on Regularized Least Square_
  http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf

~~~
- Implemented by `sklearn.linear_model.RidgeCV`.

---
section: Perceptron
# Binary Classification

Binary classification is a classification in two classes.

~~~
To extend linear regression to binary classification, we might seek
a **threshold** and then classify an input as negative/positive
depending whether $y(→x; →w) = →x^T→w + b$ is smaller/larger than a given threshold.

~~~
Zero value is usually used as the threshold, both because of symmetry and also
because the **bias** parameter acts as a trainable threshold anyway.

---
# Binary Classification

![w=50%,f=right](binary_classification.svgz)

- Consider two points on the decision boundary. Because $y(→x_1; →w)=y(→x_2; →w)$,
  we have $(→x_1-→x_2)^T→w=0$, and so $→w$ is orthogonal to every vector on the
  decision surface – $→w$ is a **normal** of the boundary.

~~~
- Consider $→x$ and let $→x_⊥$ be orthogonal projection of $x$ to the bounary,
  so we can write $→x=→x_⊥ + r\frac{→w}{||→w||}$. Multiplying both sides
  by $→w^T$ and adding $b$, we get that the distance of $→x$ to the boundary is
  $r=\frac{y(→x)}{||→w||}$.

~~~
- The distance of the decision boundary from origin is therefore
  $\frac{|b|}{||→w||}$.


---
# Perceptron

The perceptron algorithm is probably the oldest one for training
weights of a binary classification. Assuming the target value
$t ∈ \{-1, +1\}$, the goal is to find weights $→w$ such that
for all train data,
$$\operatorname{sign}(y(→x_i; →w)) = \operatorname{sign}(→w^T →x_i) = t_i,$$
or equivalently,
$$t_i y(→x_i; →w) = t_i →w^T →x_i > 0.$$

~~~

![w=60%,mw=35%,h=center,f=right](linearly_separable.svgz)

Note that a set is called **linearly separable**, if there exists
a weight vector $→w$ such that the above equation holds.

---
# Perceptron

The perceptron algorithm was invented by Rosenblat in 1958.

<div class="algorithm">

**Input**: Linearly separable dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{-1, +1\}^N$).<br>
**Output**: Weights $→w ∈ ℝ^D$ such that $t_i →x_i^T→w > 0$ for all $i$.

- $→w ← 0$
- until all examples are classified correctly, process example $i$:
  - $y ← →w^T→x_i$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We will prove that the algorithm always arrives at some correct set of
weights $→w$ if the training set is linearly separable.

---
# Perceptron as SGD

Consider the main part of the perceptron algorithm:

<div class="algorithm">

  - $y ← →w^T→x_i$
  - if $t_i y ≤ 0$ (incorrectly classified example):
    - $→w ← →w + t_i →x_i$
</div>

~~~
We can derive the algorithm using on-line gradient descent, using
the following loss function
$$L(y(→x; →w), t) ≝ \begin{cases} -t →x^T →w & \textrm{if~}t →x^T →w ≤ 0 \\ 0 & \textrm{otherwise}\end{cases}
  = \max(0, -t→x^T →w) = \ReLU(-t→x^T →w).$$

~~~
In this specific case, the value of the learning rate does not actually matter,
because multiplying $→w$ by a constant does not change a prediction.

---
# Perceptron Example

![w=52%,h=center](perceptron_convergence.svgz)

---
class: dbend
# Proof of Perceptron Convergence

Let $→w_*$ be some weights separating the training data and let $→w_k$ be the
weights after $k$ non-trivial updates of the perceptron algorithm, with $→w_0$
being 0.

~~~
We will prove that the angle $α$ between $→w_*$ and $→w_k$ decreases at each step.
Note that
$$\cos(α) = \frac{→w_*^T →w_k}{||→w_*||⋅||→w_k||}.$$

---
class: dbend
# Proof of Perceptron Convergence

Assume that the maximum norm of any training example $||→x||$ is bounded by $R$,
and that $γ$ is the minimum margin of $→w_*$, so $t →w_*^T →x ≥ γ.$

~~~
First consider the dot product of $→w_*$ and $→w_k$:
$$→w_*^T →w_k = →w_*^T (→w_{k-1} + t_k →x_k) ≥ →w_*^T →w_{k-1} + γ.$$
By iteratively applying this equation, we get
$$→w_*^T →w_k ≥ kγ.$$

~~~
Now consider the length of $→w_k$:
$$\begin{aligned}
||→w_k||^2 &= ||→w_{k-1} + t_k→x_k||^2 = ||→w_{k-1}||^2 + 2t→w_{k-1}^T→x_k + ||→x_k||^2
\end{aligned}$$

~~~
Because $→x_k$ was misclassified, we know that $t →w_{k-1}^T →x_k ≤ 0$, so
$||→w_k||^2 ≤ ||→w_{k-1}||^2 + R^2.$
~~~
When applied iteratively, we get $||→w_k||^2 ≤ k ⋅ R^2$.

---
class: dbend
# Proof of Perceptron Convergence

Putting everything together, we get
$$\cos(α) = \frac{→w_*^T →w_k}{||→w_*||⋅||→w_k||} ≥ \frac{kγ}{\sqrt{kR^2}||→w_*||}.$$

~~~
Therefore, the $\cos(α)$ increases during every update. Because the value of
$\cos(α)$ is at most one, we can compute the upper bound on the number
of steps when the algorithm converges as
$$1 ≤ \frac{\sqrt{k}γ}{\sqrt{R^2}||→w_*||}\textrm{~or~}k ≥ \frac{R^2||→w_*||^2}{γ^2}.$$

---
# Perceptron Issues

Perceptron has several drawbacks:
- If the input set is not linearly separable, the algorithm never finishes.
~~~
- The algorithm cannot be easily extended to classification into more than two
  classes.
~~~
- The algorithm performs only prediction, it is not able to return the
  probabilities of predictions.
~~~
- Most importantly, Perceptron algorithm finds _some_ solution, not necessary
  a good one, because once it finds some, it cannot perform any more updates.

![w=50%,h=center](perceptron_suboptimal.svgz)

---
section: ProbabilityBasics
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $φ ∈ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
![w=50%,f=right](bernoulli_variance.svgz)

$$\begin{aligned}
  P(x) &= φ^x (1-φ)^{1-x} \\
  𝔼[x] &= φ \\
  \Var(x) &= φ(1-φ)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $→p ∈ [0, 1]^k$ such that $∑_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(→x) &= ∏\nolimits_i^k p_i^{x_i} \\
  𝔼[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
# Information Theory

## Self Information

Amount of **surprise** when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have **additive** information.

~~~
$$I(x) ≝ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of **surprise** in the whole distribution.
$$H(P) ≝ 𝔼_{⁇x∼P}[I(x)] = -𝔼_{⁇x∼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~ ~~
![w=40%,f=right](entropy_example.svgz)

- for discrete $P$: $H(P) = -∑_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -∫ P(x) \log P(x)\,\mathrm dx$

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) has slightly different semantics, for example, it can be
negative.

~~~
From now on, all logarithms are _natural logarithms_ with base $e$.

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ≝ -𝔼_{⁇x∼P}[\log Q(x)]$$

~~~
### Gibbs Inequality
- $H(P, Q) ≥ H(P)$
~~~
- $H(P) = H(P, Q) ⇔ P = Q$
~~~

Proof: Consider $H(P) - H(P, Q) = ∑_x P(x) \log \frac{Q(x)}{P(x)}.$

~~~
Using the fact that $\log x ≤ (x-1)$ with equality only for $x=1$, we get
$$∑_x P(x) \log \frac{Q(x)}{P(x)} ≤ ∑_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = ∑_x Q(x) - ∑_x P(x) = 0.$$

~~~
For the equality to hold, $\frac{Q(x)}{P(x)}$ must be 1 for all $x$, i.e., $P=Q$.

---
# Information Theory

## Corollary of the Gibbs inequality

For a categorical distribution with $n$ outcomes, $H(P) ≤ \log n$, because for
$Q(x) = 1/n$ we get $H(P) ≤ H(P, Q) = -∑_x P(x) \log Q(x) = \log n.$

~~~
## Nonsymmetry

Note that generally $H(P, Q) ≠ H(Q, P)$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called **relative entropy**.

$$D_\textrm{KL}(P || Q) ≝ H(P, Q) - H(P) = 𝔼_{⁇x∼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) ≥ 0$
~~~
- generally $D_\textrm{KL}(P || Q) ≠ D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.svgz)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $μ$ and variance $σ^2$:
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right)$$

For standard values $μ=0$ and $σ^2=1$ we get $𝓝(x; 0, 1) = \sqrt{\frac{1}{2π}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.svgz)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with **maximum entropy**
is exactly the normal distribution.

---
section: MLE
# Maximum Likelihood Estimation


Let $𝕏 = \{→x_1, →x_2, …, →x_N\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $p̂_\textrm{data}$.

~~~
Let $p_\textrm{model}(→x; →w)$ be a family of distributions.
~~~
The **maximum likelihood estimation** of $→w$ is:

$$\begin{aligned}
→w_\mathrm{ML} &= \argmax_→w p_\textrm{model}(\mathbb X; →w) \\
               &= \argmax_→w ∏\nolimits_{i=1}^N p_\textrm{model}(→x_i; →w) \\
               &= \argmin_→w ∑\nolimits_{i=1}^N -\log p_\textrm{model}(→x_i; →w) \\
               &= \argmin_→w 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(→x; →w)] \\
               &= \argmin_→w H(p̂_\textrm{data}, p_\textrm{model}(→x; →w)) \\
               &= \argmin_→w D_\textrm{KL}(p̂_\textrm{data}||p_\textrm{model}(→x; →w)) \color{gray} + H(p̂_\textrm{data})
\end{aligned}$$

---
# Maximum Likelihood Estimation

MLE can be easily generalized to a conditional case, where our goal is to predict $t$ given $→x$:
$$\begin{aligned}
→w_\mathrm{ML} &= \argmax_→w p_\textrm{model}(\mathbb T | \mathbb X; →w) \\
               &= \argmax_→w ∏\nolimits_{i=1}^m p_\textrm{model}(t_i | →x_i; →w) \\
               &= \argmin_→w ∑\nolimits_{i=1}^m -\log p_\textrm{model}(t_i | →x_i; →w) \\
\end{aligned}$$

~~~
The resulting _loss function_ is called _negative log likelihood_, or
_cross-entropy_ or _Kullback-Leibler divergence_.

---
class: dbend
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model<br>
family $p_\textrm{model}(⋅; →w)$. Furthermore, assume there exists a unique
$→w_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(⋅; →w_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $→w_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $→w_m$ converges in probability to
  $→w_{p_\textrm{data}}$.

  Formally, for any $ε > 0$, $P(||→w_m - →w_{p_\textrm{data}}|| > ε) → 0$
  as $m → ∞$.

~~~
- MLE is in a sense most _statistically efficient_. For any consistent estimator, we
  might consider the average distance of $→w_m$ and $→w_{p_\textrm{data}}$,
  formally $𝔼_{⁇→x_1, \ldots, ⁇→x_m ∼ p_\textrm{data}} [||→w_m - →w_{p_\textrm{data}}||^2]$.
  It can be shown (Rao 1945, Cramér 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
section: LogisticRegression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  p(C_1 | →x) &= σ(→x^T →w + b) \\
  p(C_0 | →x) &= 1 - p(C_1 | →x),
\end{aligned}$$
where $σ$ is a **sigmoid function**
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
It can be trained using the SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$σ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$σ'(x) = σ(x) \big(1 - σ(x)\big)$$

~~~
![w=100%](sigmoid.svgz)

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$p(C_1 | →x) = σ(y(→x; →w)) = \frac{1}{1 + e^{-y(→x; →w)}}$$
~~~
we can arrive at
$$y(→x; →w) = \log\left(\frac{p(C_1 | →x)}{p(C_0 | →x)}\right),$$
where the prediction of the model $y(→x; →w)$ is called a **logit**
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(→x; →w) = →x^T →w$, we use MLE (the maximum likelihood
estimation). Note that $p(C_1 | →x; →w) = σ(y(→x; →w))$.

~~~
Therefore, the loss for a batch $𝕏=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
𝓛(𝕏) = \frac{1}{N} ∑_i -\log(p(C_{t_i} | →x_i; →w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}^N$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(p(C_{t_i} | →x_i; →w))$
  - $→w ← →w - α→g$
</div>

---
# Linearity in Logistic Regression

![w=100%](lr_linearity.svgz)
