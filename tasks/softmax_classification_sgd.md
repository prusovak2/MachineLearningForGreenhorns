### Assignment: softmax_classification_sgd
#### Date: Deadline: Nov 10, 23:59
#### Points: 3 points
#### Examples: softmax_classification_sgd_examples

Starting with the [softmax_classification_sgd.py](https://github.com/ufal/npfl129/tree/master/labs/04/softmax_classification_sgd.py),
implement minibatch SGD for multinomial logistic regression.

#### Examples Start: softmax_classification_sgd_examples
Note that your results may sometimes be slightly different (for example because of varying floating point arithmetic on your CPU).
- `python3 softmax_classification_sgd.py --batch_size=10  --iterations=10 --learning_rate=0.005`
```
After iteration 1: train loss 0.2428 acc 93.4%, test loss 0.2996 acc 90.8%
After iteration 2: train loss 0.1962 acc 94.6%, test loss 0.2649 acc 92.5%
After iteration 3: train loss 0.1746 acc 95.2%, test loss 0.2595 acc 91.5%
After iteration 4: train loss 0.1268 acc 96.8%, test loss 0.2074 acc 92.1%
After iteration 5: train loss 0.1013 acc 97.4%, test loss 0.1861 acc 93.9%
After iteration 6: train loss 0.0950 acc 98.4%, test loss 0.1754 acc 93.7%
After iteration 7: train loss 0.0810 acc 98.1%, test loss 0.1587 acc 94.9%
After iteration 8: train loss 0.0761 acc 98.2%, test loss 0.1564 acc 94.9%
After iteration 9: train loss 0.0764 acc 98.3%, test loss 0.1654 acc 94.9%
After iteration 10: train loss 0.0746 acc 98.2%, test loss 0.1694 acc 95.2%
Learned weights:
  -0.03 -0.10 0.01 0.07 -0.03 0.03 -0.07 0.05 0.07 -0.10 ...
  0.09 0.08 -0.12 -0.07 -0.20 0.10 -0.02 -0.06 0.02 -0.07 ...
  0.05 0.07 0.01 0.01 -0.03 0.02 0.01 -0.10 -0.03 0.10 ...
  0.02 -0.05 -0.03 0.09 0.17 0.14 -0.02 0.05 -0.09 0.04 ...
  -0.07 -0.07 -0.11 -0.06 -0.09 -0.09 -0.10 0.03 -0.04 0.01 ...
  -0.07 -0.04 0.18 0.02 0.04 0.14 0.10 0.03 -0.03 0.02 ...
  -0.09 -0.04 -0.12 -0.08 -0.08 -0.12 -0.08 0.05 0.05 -0.05 ...
  0.07 0.02 0.04 -0.02 0.10 0.01 0.16 -0.04 0.03 0.01 ...
  0.02 -0.02 0.02 -0.05 -0.02 -0.03 -0.10 -0.03 0.08 -0.07 ...
  0.04 -0.06 -0.07 0.10 -0.04 -0.05 0.06 -0.08 -0.01 -0.01 ...
```
- `python3 softmax_classification_sgd.py --batch_size=1   --iterations=10 --learning_rate=0.005 --test_size=1597`
```
After iteration 1: train loss 0.9709 acc 83.0%, test loss 1.5315 acc 76.8%
After iteration 2: train loss 0.5639 acc 90.0%, test loss 1.2635 acc 84.2%
After iteration 3: train loss 0.9004 acc 83.5%, test loss 1.4205 acc 80.3%
After iteration 4: train loss 0.0646 acc 97.5%, test loss 0.8086 acc 88.7%
After iteration 5: train loss 0.0475 acc 98.5%, test loss 0.7024 acc 90.7%
After iteration 6: train loss 0.0711 acc 97.0%, test loss 0.8800 acc 88.5%
After iteration 7: train loss 0.3303 acc 92.5%, test loss 1.1663 acc 85.3%
After iteration 8: train loss 0.0862 acc 97.5%, test loss 0.9596 acc 87.5%
After iteration 9: train loss 0.0050 acc 100.0%, test loss 0.7205 acc 90.5%
After iteration 10: train loss 0.2158 acc 95.0%, test loss 1.1674 acc 87.0%
Learned weights:
  -0.03 -0.10 0.04 0.15 -0.01 0.05 -0.07 0.05 0.07 -0.12 ...
  0.09 0.06 -0.37 -0.24 -0.43 0.17 0.06 -0.06 0.02 -0.17 ...
  0.05 0.12 0.20 0.09 0.03 0.15 -0.01 -0.10 -0.03 0.25 ...
  0.02 -0.01 0.10 0.37 0.60 0.13 -0.16 0.06 -0.09 0.30 ...
  -0.07 -0.06 -0.22 -0.20 -0.26 -0.28 -0.10 0.04 -0.04 -0.03 ...
  -0.07 -0.07 0.49 0.25 0.33 0.41 0.40 0.05 -0.03 -0.05 ...
  -0.09 -0.07 -0.33 -0.28 -0.07 -0.30 -0.10 0.05 0.05 -0.14 ...
  0.07 -0.00 -0.15 -0.02 0.16 0.18 0.19 -0.09 0.03 -0.09 ...
  0.02 -0.01 -0.08 -0.11 -0.08 -0.10 -0.29 -0.03 0.08 -0.12 ...
  0.04 -0.05 0.12 -0.00 -0.46 -0.26 0.01 -0.08 -0.01 0.08 ...
```
- `python3 softmax_classification_sgd.py --batch_size=100 --iterations=10 --learning_rate=0.05`
```
After iteration 1: train loss 4.6130 acc 66.4%, test loss 4.7159 acc 66.5%
After iteration 2: train loss 0.4662 acc 91.0%, test loss 0.5955 acc 87.1%
After iteration 3: train loss 0.4140 acc 90.2%, test loss 0.6039 acc 87.5%
After iteration 4: train loss 0.2332 acc 93.3%, test loss 0.4390 acc 90.0%
After iteration 5: train loss 0.1398 acc 96.2%, test loss 0.2685 acc 91.8%
After iteration 6: train loss 0.0965 acc 96.9%, test loss 0.2300 acc 93.7%
After iteration 7: train loss 0.1034 acc 96.7%, test loss 0.2712 acc 92.5%
After iteration 8: train loss 0.2048 acc 93.8%, test loss 0.4191 acc 90.5%
After iteration 9: train loss 0.8357 acc 84.6%, test loss 0.9188 acc 84.1%
After iteration 10: train loss 0.0825 acc 97.7%, test loss 0.2471 acc 94.0%
Learned weights:
  -0.03 -0.11 -0.02 0.08 -0.08 -0.04 -0.10 0.05 0.07 -0.12 ...
  0.09 0.07 -0.15 -0.14 -0.27 0.15 -0.05 -0.07 0.02 -0.11 ...
  0.05 0.10 0.10 0.03 -0.04 0.05 0.03 -0.10 -0.03 0.20 ...
  0.02 -0.04 -0.00 0.12 0.26 0.24 -0.02 0.05 -0.09 0.08 ...
  -0.07 -0.07 -0.15 -0.14 -0.08 -0.14 -0.13 0.03 -0.04 0.00 ...
  -0.07 -0.03 0.29 0.07 0.10 0.27 0.20 0.03 -0.03 0.01 ...
  -0.09 -0.05 -0.27 -0.12 -0.21 -0.36 -0.15 0.05 0.04 -0.12 ...
  0.07 0.01 0.05 -0.00 0.14 0.09 0.23 -0.04 0.03 0.01 ...
  0.02 -0.03 0.02 -0.04 0.02 -0.06 -0.16 -0.03 0.08 -0.06 ...
  0.04 -0.06 -0.05 0.15 -0.03 -0.06 0.07 -0.07 -0.01 -0.01 ...
```
#### Examples End:
